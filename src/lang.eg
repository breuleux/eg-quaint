
require:
   opg ->
      Source, Location, transferLocation as [<<:]
      tokenize as tknz ->
         Tokenizer, indentTracker, inferLocation
      parse as _parse ->
         Parser, TokenGroups, PriorityOrder

provide:
   tokenizer, tokenize
   parser, parse


tokenizer = Tokenizer with {

   regexps = {
      {.nl, " *(?:\n *)+"}
      {.ws, "[ ~]+"}
      {.op, "[!@#$%^&*_\\-+=<>/?;:.`|]+|[(){}\\[\\],.]"}
      {.word, "(?:\\\\.|[^\n!@#$%^&*_\\-+=<>/?;:.`|\\[\\](){},. ~])+"}
      {.unknown, "."} ;; should never happen
   }

   post = {
      indentTracker{calcSize}
      inferLocation
      addDefaults
      tuckWhitespace
      classifyOperators
      padLocations
   } where

      calcSize{match token} =
         {type => .nl} ->
            {*, x} = token.token.split{"\n"}
            x.length
         else ->
            -1

      addDefaults{token} =
         token &: {wsb = 0, wsa = 0}

      tuckWhitespace{match tok, i, toks} =
         do:
            {prev, next} = {toks[i-1], toks[i+1]}
         {type => .ws, => token} ->
            prev.wsa and next.wsb = token.length
            next.tucked = tok
            {}
         {type => .op, token => "]" or "}" or ")"} ->
            ;; prev.wsa and prev.wsb = 1
            prev.wsa = 1
            {tok}
         {type => [.indent or .nl]} ->
            prev.wsa and prev.wsb = 1
            {tok}
         {type => .boundary} when prev ->
            ;; prev.wsa and prev.wsb = 1
            prev.wsa = 1
            {tok}
         else ->
            {tok}

      classifyOperators{match tok} =
         do:
            tuck = tok.tucked
            delete tok.tucked
            sfx = this.sfx
            pad{tok} =
               if sfx:
                  padder = tuck or {
                     type = .sh_ws, token = ""
                     location = Location{
                        tok.location.source
                        tok.location.start
                        tok.location.start
                     }
                  }
                  {padder, tok}
               else:
                  {tok}

         {type => [.boundary or .ws or .indent]} ->
            this.sfx = false
            {tok}

         {type => .nl, => token} ->
            this.sfx = false
            if token.split{"\n"}.length <= 2:
               tok.type = .sh_nl
            {tok}

         {type => .dedent} ->
            this.sfx = true
            {tok}

         {type => .word} ->
            this.sfx = true
            pad{tok}

         {type => .op, => token, => wsb, => wsa} ->
            var never_sfx = false
            match tok.token:
               "(" or "[" or "{" ->
                  tok.type = .open
                  this.sfx = false
                  return pad{tok}
               ")" or "]" or "}" ->
                  tok.type = .close
                  this.sfx = true
                  return {tok}
               "," or ":" ->
                  never_sfx = true
               else ->
                  pass

            {tok.type, this.sfx, get} =
               match {sfx, wsb, wsa}:
                  {true?,  0  , 0}     -> {.sh_ifx, false, {tok}}
                  {true?,  > 0, > 0}   -> {.wi_ifx, false, {tok}}
                  {true?,  _  , 0}     -> {.sh_pfx, false, pad{tok}}
                  {true?,  0  , _}     -> {if{never_sfx, .wi_ifx, .sh_sfx}
                                           not never_sfx, {tok}}
                  {false?, 0  , 0}     -> {.sh_pfx, false,  {tok}}
                  {false?, > 0, > 0}   -> {.wi_pfx, false,  {tok}}
                  {false?, _  , 0}     -> {.sh_pfx, false,  {tok}}
                  {false?, 0  , _}     -> {.wi_pfx, false,  {tok}}

         else ->
            throw E.unknown_token_type{tok.type + " " + tok.token}

      padLocations{tok, i, toks} =
         ;; if toks[i - 1]:
         ;;    tok.location.start = toks[i - 1].location.end
         tok

}


prio = PriorityOrder{tokgrp, priorities} where

   tokgrp = TokenGroups with {
      sh_pfx = {.sh_pfx}
      sh_ifx = {.sh_ifx}
      sh_sfx = {.sh_sfx}
      wi_pfx = {.wi_pfx}
      wi_ifx = {.wi_ifx}
      wi_sfx = {.wi_sfx}
      sh_ws = {.sh_ws}
      ws = {.ws}
      sh_nl = {.sh_nl}
      nl = {.nl}
      indent = {.indent}
      dedent = {.dedent}
      openp = {"open ("}
      closep = {"close )"}
      opens = {"open ["}
      closes = {"close ]"}
      openb = {"open {"}
      closeb = {"close }"}
      period = {"sh_sfx ."}
      word = {.word}
      boundary = {.boundary}
   }

   MAX = Infinity

   priorities = {
      indent = {left = 280, right = 1}
      dedent = {left = 1  , right = MAX}
      opens  = {left = MAX, right = 2}
      closes = {left = 2  , right = MAX}
      openb  = {left = MAX, right = 3}
      closeb = {left = 3  , right = MAX}
      openp  = {left = MAX, right = 4}
      closep = {left = 4  , right = MAX}

      nl     = {left = 100, right = 100}
      sh_nl  = {left = 150, right = 150}

      wi_pfx = {left = MAX, right = 200}
      wi_ifx = {left = 201, right = 200}
      wi_sfx = {left = 201, right = MAX}
      ws     = {left = 250, right = 250}

      period = {left = 299, right = MAX}
      sh_pfx = {left = MAX, right = 300}
      sh_ifx = {left = 301, right = 300}
      sh_sfx = {left = 301, right = MAX}
      sh_ws  = {left = 350, right = 350}

      word   = {left = 1000, right = 1000}
      boundary = {left = -1, right = -1}
   }


finalize{match node} =
   {null?, mid and {=> token, => location}, null?} ->
      #text{token} <<: location

   {*parts} ->
      newparts = enumerate{parts} each {match i, part} ->
         match is part when i mod 2 == 0 ->
            null? ->
               #text{""} <<:
                  Location{if{p0, p0.location.source, p1.location.source}, s, e} where
                     p0 = parts[i - 1]
                     p1 = parts[i + 1]
                     s = if{p0, p0.location.end, p1.location.start}
                     e = if{p1, p1.location.start, p0.location.end}

                  ;; if [p = parts[i + 1]]:
                  ;;    p.location.at_start{}
                  ;; else:
                  ;;    parts[i - 1].location.at_end{}
            else ->
               part
         else ->
            loc = Location{
               part.location.source
               if{let p = parts[i - 1], p.location.end, part.location.start}
               if{let p = parts[i + 1], p.location.start, part.location.end}
            }
            {let rval, width} = match part.type:
               .indent  -> {#oper{"I("}, .wide}
               .dedent  -> {#oper{")I"}, .wide}
               .sh_nl   -> {#oper{"NL"}, .short}
               .nl      -> {#oper{"NL"}, .wide}
               .sh_ws   -> {#oper{"J"}, .short}
               .ws      -> {#oper{"J"}, .wide}
               R"^sh_"? -> {#oper{part.token}, .short}
               R"^wi_"? -> {#oper{part.token}, .wide}
               else     -> {#oper{part.token}, .wide}
            rval.width = width
            rval <<: loc

      rval = #seq{*newparts} <<: Location{
         newparts[0].location.source
         newparts[0].location.start
         newparts[newparts.length - 1].location.end
      }
      rval &: {
         operator = newparts[1][1]
         width = newparts[1].width
      }
      adjust{rval}
   else ->
      throw E.syntax.unknown_node{node}


adjust{match node} =
   #seq{pre, inds and #oper{"I("}, mid, inde and #oper{")I"}, tail} ->
      findTarget{match x} =
         #seq{pre, #oper{"NL"}, *} ->
            findTarget{pre} or x
         else ->
            false
      target = findTarget{mid}
      if target:
         loc = pre.location.at_start{}
         target.location.start = loc.start
         node.location.start = loc.start
         inds.location = loc
         node[1] = #text{""} <<: loc
         target[1] = pre
      node
   else ->
      node

parser = Parser{tokenizer, prio.getOrder{}, finalize}


tokenize{src} =
   tokenizer.run{src}

parse{src} =
   parser.parse{src}

